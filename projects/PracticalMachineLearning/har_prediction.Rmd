---
title: "Human Activity Recognition Prediction Model"
author: "Mitch Murphy"
date: "5/10/2018"
output: html_document
---

```{r setup, include=FALSE, echo = FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(caret); library(ggplot2); library(gbm); library(randomForest)
library(gridExtra); library(VIM); library(parallel); library(doParallel)
setwd("~/Desktop/coursera/DataScience/projects/PracticalMachineLearning/")
```

## Abstract
Data is king, and human activity is no exception. Devices such as Fitbit now provide means to easily collect large 
amounts of personal activity data. In this case study, the data we utilize comes from accelerometers on the belt, 
forearm, arm, and dumbbell of 6 individuals, and our goal is to accurately predict the activity they were attempting to 
perform. It always helps to first visualize your data which can lead you to be able to identify the most "important" 
variables/features. Clustering is another way of carrying out dimensionality reduction, which is of particular 
importance when computational power is expensive, or the data becomes too large. The model I came up with has an 
out-of-bag (OOB) error rate of 0.64%, and classified the validation set perfectly (accuracy, sensitivity and specificity 
are all 100%). I breifly touch on dimensionality reduction, combining 5 variables into 1 cluster, with this model only 
increasing it's OOB error rate to 0.84%, but still performed perfectly on the validation set. Perfect performance leads 
me to suspect that these models' are overfitting; reducing the size of the train set should affect the performance on 
the validation set.


## Methods
The data used in this study can be found here: http://groupware.les.inf.puc-rio.br/har. For reproducibility reasons, 
let's set the seed. Let's also take advantage of parallelization:
```{r seed}
setup_start_time <- Sys.time()
set.seed(1234)
```

### Data Preparation
The data is comprised of 160 variables: the activity classe (y) plus 159 predictors. Upon closer inspection, the 
majority of these predictors are statistical metrics (calculated on the data, at various time intervals), as well as an 
index (X), the user's name, and some timestamp information.

```{r dataLoad}
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(trainURL)
testing <- read.csv(testURL)
```
As suspected, the majority of variable observations are missing with only about 2% of all variables being complete. As 
stated above, there are over 100 computed metrics (like standard deviation), so let's remove these.

```{r dataPrep}
varsToIgnoreRegEx <- "^(X|user_name|raw_timestamp_part_1|raw_timestamp_part_2|cvtd_timestamp|new_window|num_window|kurtosis|skewness|min|max|stddev|total|var|avg|ampl)"
colsToIgnore <- grep(varsToIgnoreRegEx, names(training))
training <- training[,-colsToIgnore]
testing <- testing[,-colsToIgnore]
table(complete.cases(training))
```

Now every observation is complete! The timestamp variables are used to determine when to compute metrics on the data, 
and while we ignore these metrics it would be nice to have an elapsed time variable. Now we will split the training data 
into train and validation sets (80/20 split):

```{r splitData}
inTrain <- createDataPartition(y=training$classe, p=0.8, list=F)
# training on training :)
training <- training[inTrain,]; validation <- training[-inTrain,]
setup_end_time <- Sys.time()
#setup time
print(setup_end_time - setup_start_time)
```
 
We are now ready to build our model. The model is built using the random forest classification algorithm, and we 
manually specify the tuning parameters to use 3-fold cross-validation as well as parallelization (which will 
dramatically speed up the training phase).

```{r buildModel, cache=TRUE}
# first check if the model has been saved, if so no need to train it again
modelFilename <- "modFit.Rdata"
if(modelFilename %in% dir()) {
  load(modelFilename)
} else {
  train_start_time <- Sys.time()
  cl <- makeCluster(detectCores() - 1)
  registerDoParallel(cl)
  myTrainControl <- trainControl(method="cv", 
                                 number=3, 
                                 allowParallel = TRUE)
  modFit <- train(classe ~ ., data=training, method="rf", trControl=myTrainControl)
  train_end_time <- Sys.time()
  print(train_end_time - train_start_time)
  stopCluster(cl)
  save(modFit, file = modelFilename)
}

print(modFit$finalModel)
```

In this example, we used the default of 500 trees, but we can optimize this by plotting the accuracy vs. # of 
predictors (as shown below). This plot tells us that the maximum accuracy occurs at just around 24 trees. Now that we 
have constructed the model, we can make predictions on the test set (in this case the validation set).


## Results
```{r predictions}
prediction <- predict(modFit, newdata=validation)
confusionMatrix(validation$classe, prediction)
test.pred <- predict(modFit, newdata = testing)
print(test.pred)
```
We built a perfect classifier! While this is very good, it leaves me quite skeptical that this model suffers from 
overfitting. It would be nice to expand on this by tuning the train/validation set split and measure the compare the 
performance of the various parameters. 

## Conclusion
We can develop a highly accurate model to predict activity class based on sensor data, using the random forest 
algorithm. Using this model, we made predictions on the 20 test observations:

```{r conclusion}
test.pred <- predict(modFit, newdata = testing)
print(test.pred)
```

Our model was 100% accurate in predicting the test set! The simplified model (below) also classified the test cases 
perfectly.


## Further Discussion
We can leverage PCA as well as clustering for dimensionality reduction. First let us compute the importance of each of 
the predictors. The roll_belt variable is by far the most "important." Using this information in addition to some 
PCA we can get a better idea of which variables we should try and cluster.
```{r varImportance}
var.importance <- varImp(modFit$finalModel)
var.importance$name <- rownames(var.importance)
var.importance <- var.importance[order(var.importance$Overall, decreasing = T),]
rownames(var.importance) <- NULL
print(var.importance)
qplot(data=training,x=roll_belt,y=pitch_forearm,col=training$classe)
```


### PCA

```{r plotPCA}
plotPCAForVariable <- function(var, varname) {
  qplot(data=training_pc12,x=x,y=y,col=training[,c(as.numeric(var))]) + labs(colour = varname)
}
training.scaled <- scale(training[,-c(49)], center=T,scale=T)
pc <- prcomp(training.scaled) 
cumsum((pc$sdev^2 / sum(pc$sdev^2))) # First 2 principle components exlain ~ 47% of the variance 
training_pc12 <- as.matrix(training.scaled) %*% pc$rotation[,c(1,2)]
training_pc12 <- data.frame(x=training_pc12[,1],y=training_pc12[,2])
plotPCAForVariable(49, "Activity Class")
```
So about 47% of all variability in the data can be explained by the first 2 principal components. We color these points 
based on their activity class. We furthered this mini analysis by coloring the data on a few other variables - those 
being the top 3 in importance (that we computed earlier). 
```{r roll_belt}
plotPCAForVariable(1,"roll_belt")
```
According to this, roll_belt looks to be a promising variable to "collapse" into a factor variable with just 2 levels.

```{r pitch_forearm}
plotPCAForVariable(38, "pitch_forearm")
```
pitch_forearm does not look to be a good candidate (without any transformations). 

```{r yaw_belt}
plotPCAForVariable(3,"yaw_belt")
```

At first glance, it looks like yaw_belt can be split by the sign of the first 2 principal components.

### Clustering
Clustering is also a very good way to reduce the features of a data set. Here we simply create one cluster, replace n 
features with the cluster number and observe how it affects the performance of our model. 
```{r clustering, cache=TRUE}
cm <- cor(training[,-c(49)])
# from the correlation matrix lets identify variables to cluster on
# 1 and 3,7:9 (roll_belt, accel_belt_x, accel_belt_y, accel_belt_z)

clusterInfoFilename <- "clusterInfo.Rdata"
if(clusterInfoFilename %in% dir()) {
  load(clusterInfoFilename)
  train.kmeans <- clusterInfo$train.kmeans
  validation.kmeans <- clusterInfo$validation.kmeans
} else {
  dist1 <- dist(training[,colsToCombine])
  clust1 <- hclust(dist1)
  kmeans1 <- kmeans(training[,colsToCombine], centers = 180)
  dist2 <- dist(validation[,colsToCombine])
  clust2 <- hclust(dist2)
  kmeans2 <- kmeans(validation[,colsToCombine], centers = 180)
  clusterInfo <- list(train.kmeans = kmeans1, validation.kmeans = kmeans2)
  save(clusterInfo, file = clusterInfoFilename)
}

# take a look at the dendrogram in order to roughly identify where to cut the tree (number of centers)
#plot(clust1)
# so from the dendrogram, it looks like we might be able to reduce 5 variables into 1 (cluster), 
# where a good cut might be at k=180 (so the new variable is between 1 and 180)

training$new_belt <- clusterInfo$train.kmeans$cluster
validation$new_belt <- clusterInfo$validation.kmeans$cluster

newModelFilename <- "modFit2.Rdata"
if(newModelFilename %in% dir()) {
  load(newModelFilename)
} else {
  modFit2 <- train(classe ~., data = training[,-colsToCombine], method="rf", trControl=myTrainControl)
  save(modFit2, file = newModelFilename)
}
print(modFit2$finalModel)
prediction2 <- predict(modFit2, newdata=validation)
confusionMatrix(validation$classe, prediction2)
```
This very simple clustering technique reduced the number of variables from 5 down to 1, increased the OOB error rate 
from 0.64% to 0.88%, slightly reduced accuracy to 99.9%, sensitivity to 99.9% and specificity to 99.974%.
This strategy should be further employed on sets of other highly correlated variables.
