{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:24:52.794431Z",
     "start_time": "2018-12-05T03:24:52.200610Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:24:52.906271Z",
     "start_time": "2018-12-05T03:24:52.892218Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_review(review):\n",
    "    review = review.lower().replace(\"'\", \"\")\n",
    "    review = review.replace('\"', '')\n",
    "    review = re.sub('[\\[\\]\\(\\)\\:\\;\\.,]',' ',review)\n",
    "    review = re.split('[\\/<\\?!\\s]',review)\n",
    "    return [token.strip() for token in review if token.strip() != '']\n",
    "\n",
    "def preprocess_review(review):\n",
    "    review = review.lower()#.replace(\"'\", \"\")\n",
    "    review = review.replace('\"', '')\n",
    "    review = re.sub('[\\[\\]\\(\\)\\:\\;,]',' ',review)\n",
    "    review = re.sub('[\\/<\\?!\\.]','',review)\n",
    "    return review\n",
    "\n",
    "def build_evaluate_model(x, y, print_stats):\n",
    "    data = x\n",
    "    target = y\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb.fit(data, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    #y_pred = bnb.predict(data)\n",
    "    y_pred = cross_val_predict(bnb, data, target)\n",
    "\n",
    "    n = data.shape[0]\n",
    "    # Display our results.\n",
    "    if print_stats:\n",
    "        print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "            n,\n",
    "            (target != y_pred).sum()\n",
    "        ))\n",
    "\n",
    "    correct = (n - (target != y_pred).sum())/n * 100\n",
    "    if print_stats:\n",
    "        print(\"Accuracy: {}%\".format(correct))\n",
    "    \n",
    "    \n",
    "    cv = cross_val_score(bnb, data, target, cv=5)\n",
    "    \n",
    "    if print_stats:\n",
    "        print(cv)\n",
    "        print(\"cv average is = {:.2f}%\".format(cv.mean()*100))\n",
    "\n",
    "        cm = confusion_matrix(target, y_pred)\n",
    "        print(cm)\n",
    "        print(\"True Positive = {:.2f}%\".format(cm[1,1]/10))\n",
    "        print(\"False Positive = {:.2f}%\".format(cm[0,1]/10))\n",
    "        print(\"True Negative = {:.2f}%\".format(cm[0,0]/10))\n",
    "        print(\"False Negative = {:.2f}%\".format(cm[1,0]/10))\n",
    "        print(\"sensitivity or hit rate is {:.2f}%\".format(cm[1,1]/(cm[1,0]+cm[1,1])*100))\n",
    "        # sensitivity (recall) is the percentage of positives identified or TP/FN+TP\n",
    "        print(\"specificity or True Negative rate is {:.2f}%\".format(cm[0,0]/(cm[0,0]+cm[0,1])*100))\n",
    "    return bnb, cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:24:53.777889Z",
     "start_time": "2018-12-05T03:24:53.760041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "reviews = pd.read_csv('./amazon_cells_labelled.txt', sep='\\t', header=None)\n",
    "reviews.columns = ['review', 'score']\n",
    "# Let's preprocess the review text\n",
    "# remove special characters\n",
    "# replace punctuation with a space\n",
    "reviews[\"review_processed\"] = reviews.review.apply(preprocess_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:24:54.864873Z",
     "start_time": "2018-12-05T03:24:54.848428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>review_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>so there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>good case  excellent value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>great for the jawbone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>the mic is great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  score  \\\n",
       "0  So there is no way for me to plug it in here i...      0   \n",
       "1                        Good case, Excellent value.      1   \n",
       "2                             Great for the jawbone.      1   \n",
       "3  Tied to charger for conversations lasting more...      0   \n",
       "4                                  The mic is great.      1   \n",
       "\n",
       "                                    review_processed  \n",
       "0  so there is no way for me to plug it in here i...  \n",
       "1                         good case  excellent value  \n",
       "2                              great for the jawbone  \n",
       "3  tied to charger for conversations lasting more...  \n",
       "4                                   the mic is great  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:24:57.918748Z",
     "start_time": "2018-12-05T03:24:57.912360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews.review_processed, reviews.score, test_size=0.3, random_state=42)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:24:58.523907Z",
     "start_time": "2018-12-05T03:24:58.465666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (negative): 758 (positive): 598\n",
      "Saturated model will have 1356 features\n"
     ]
    }
   ],
   "source": [
    "# Now tokenize each review to generate an overall list of tokens unique to each class\n",
    "# get words unique to negative reviews\n",
    "p = reviews.query(\"score == 1\").review_processed.str.split(expand=True).stack()\n",
    "n = p = reviews.query(\"score == 0\").review_processed.str.split(expand=True).stack()\n",
    "tokens_positive = set(p.values)\n",
    "tokens_negative = set(n.values)\n",
    "\n",
    "tokens_positive = []\n",
    "tokens_negative = []\n",
    "reviews.query(\"score == 1\").review_processed.apply(lambda x: tokens_positive.extend(tokenize_review(x)))\n",
    "reviews.query(\"score == 0\").review_processed.apply(lambda x: tokens_negative.extend(tokenize_review(x)))\n",
    "\n",
    "x = list(set(tokens_negative).difference(set(tokens_positive)))\n",
    "y = list(set(tokens_positive).difference(set(tokens_negative)))\n",
    "# now want to get a count of word (in x) in  tokens_negative\n",
    "# replace all numbers (ex 45, 11) with _number token\n",
    "tokens_negative = []\n",
    "tokens_positive = []\n",
    "for token in x:\n",
    "    if len(token) == 1:\n",
    "        continue\n",
    "    if token.isdigit():\n",
    "        token = \"NUMBER\"\n",
    "    if re.match('[^a-z]+[0-9]+', token):\n",
    "        token = \"NUMCHAR\"\n",
    "    if re.match('#[0-9]+', token):\n",
    "        token = \"RANK\"\n",
    "    if re.match('[a-z]\\*+', token):\n",
    "        token = \"EXPLITIVE\"\n",
    "    tokens_negative.append(token)\n",
    "\n",
    "\n",
    "for token in y:\n",
    "    if len(token) == 1:\n",
    "        continue\n",
    "    if token.isdigit():\n",
    "        token = \"NUMBER\"\n",
    "    if re.match('[^a-z]+[0-9]+', token):\n",
    "        token = \"NUMCHAR\"\n",
    "    if re.match('#[0-9]+', token):\n",
    "        token = \"RANK\"\n",
    "    if re.match('[a-z]\\*+', token):\n",
    "        token = \"EXPLITIVE\"\n",
    "    tokens_positive.append(token)    \n",
    "    \n",
    "# NUMBER is pretty balanced (12 positive, 13 negative), so lets remove that one\n",
    "tokens_negative = set(tokens_negative)\n",
    "tokens_positive = set(tokens_positive)\n",
    "tokens_negative.remove(\"NUMBER\")\n",
    "tokens_positive.remove(\"NUMBER\")\n",
    "print(\"Tokens (negative): {} (positive): {}\".format(len(tokens_negative), len(tokens_positive)))\n",
    "print(\"Saturated model will have {} features\".format(len(tokens_negative)+len(tokens_positive)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a form that is better suited for analysis, let's build some models!\n",
    "\n",
    "# Models\n",
    "\n",
    "## Model 1\n",
    "We are going to manually specify some keywords that we believe are good indicator of review sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:25:01.436037Z",
     "start_time": "2018-12-05T03:25:01.432669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's add some features that I suspect will help\n",
    "tokens_my = ['back', 'best', 'easy', 'cool', \"doesn't\",\n",
    "             'fine', 'good', 'great', 'happy', 'love',\n",
    "             'money', 'never', 'not', 'recommend', \n",
    "             'very', 'well', 'worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:25:03.935592Z",
     "start_time": "2018-12-05T03:25:03.880326Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in tokens_my:\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "    X_train[str(word)] = X_train.review_processed.str.contains(str(word), case=False)\n",
    "    X_test[str(word)] = X_test.review_processed.str.contains(str(word), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:25:05.084834Z",
     "start_time": "2018-12-05T03:25:05.053864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back</th>\n",
       "      <th>best</th>\n",
       "      <th>easy</th>\n",
       "      <th>cool</th>\n",
       "      <th>doesn't</th>\n",
       "      <th>fine</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "      <th>money</th>\n",
       "      <th>never</th>\n",
       "      <th>not</th>\n",
       "      <th>recommend</th>\n",
       "      <th>very</th>\n",
       "      <th>well</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014124</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.225989</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.031073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.046243</td>\n",
       "      <td>0.026012</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.020231</td>\n",
       "      <td>0.109827</td>\n",
       "      <td>0.182081</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.176301</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           back      best      easy      cool   doesn't      fine      good  \\\n",
       "score                                                                         \n",
       "0      0.014124  0.002825  0.008475  0.002825  0.025424  0.002825  0.025424   \n",
       "1      0.002890  0.046243  0.026012  0.017341  0.002890  0.020231  0.109827   \n",
       "\n",
       "          great     happy      love     money     never       not  recommend  \\\n",
       "score                                                                          \n",
       "0      0.005650  0.008475  0.000000  0.036723  0.011299  0.225989   0.005650   \n",
       "1      0.182081  0.028902  0.043353  0.002890  0.005780  0.034682   0.043353   \n",
       "\n",
       "           very      well     worst  \n",
       "score                                \n",
       "0      0.081921  0.011299  0.031073  \n",
       "1      0.176301  0.072254  0.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = X_train.copy()\n",
    "x2[\"score\"] = y_train\n",
    "means = x2.groupby(\"score\").mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:25:07.952360Z",
     "start_time": "2018-12-05T03:25:07.944694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "love          0.000000\n",
       "great         0.031029\n",
       "best          0.061088\n",
       "recommend     0.130320\n",
       "fine          0.139629\n",
       "well          0.156384\n",
       "cool          0.162900\n",
       "good          0.231490\n",
       "happy         0.293220\n",
       "easy          0.325800\n",
       "very          0.464666\n",
       "never         1.954802\n",
       "back          4.887006\n",
       "not           6.516008\n",
       "doesn't       8.796610\n",
       "money        12.706215\n",
       "worst              inf\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(means.iloc[0,:] / means.iloc[1,:]).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:59:45.893302Z",
     "start_time": "2018-12-05T02:59:45.886973Z"
    }
   },
   "source": [
    "*Those token ratios near 1, or inf are questionable, see if any of these need to be included.*\n",
    "\n",
    "  1. sad\n",
    "  2. avoid\n",
    "  3. awful\n",
    "  4. negative\n",
    "  5. satisfactory\n",
    "  6. quality\n",
    "  7. problem\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:25:36.519595Z",
     "start_time": "2018-12-05T03:25:36.480203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 700 points : 176\n",
      "Accuracy: 74.85714285714286%\n",
      "[0.74468085 0.75714286 0.75       0.7        0.79136691]\n",
      "cv average is = 74.86%\n",
      "[[314  40]\n",
      " [136 210]]\n",
      "True Positive = 21.00%\n",
      "False Positive = 4.00%\n",
      "True Negative = 31.40%\n",
      "False Negative = 13.60%\n",
      "sensitivity or hit rate is 60.69%\n",
      "specificity or True Negative rate is 88.70%\n"
     ]
    }
   ],
   "source": [
    "model1, cv_mean = build_evaluate_model(X_train[tokens_my], y_train, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:26:05.213735Z",
     "start_time": "2018-12-05T03:26:05.204630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 300 points : 77\n",
      "Accuracy: 74.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how it performs on the test set!\n",
    "y_pred = model1.predict(X_test.iloc[:,1:])\n",
    "\n",
    "n = 300\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        n,\n",
    "        (y_test != y_pred).sum()\n",
    "    ))\n",
    "correct = (n - (y_test != y_pred).sum())/n * 100\n",
    "print(\"Accuracy: {}%\".format(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: this model has an accuracy of **74.86%** on both the train and test sets, and is **74.33%** accurate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Now we are going to randomly select 50 features (from tokens); we sample from negative and positive tokens, and depending on the objective we vary the sampling rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:27:56.894440Z",
     "start_time": "2018-12-05T03:27:53.004215Z"
    }
   },
   "outputs": [],
   "source": [
    "for word in list(tokens_negative) + list(tokens_positive):\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "    X_train[str(word)] = X_train.review_processed.str.contains(str(word), case=False)\n",
    "    X_test[str(word)] = X_test.review_processed.str.contains(str(word), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:28:39.263660Z",
     "start_time": "2018-12-05T03:28:07.424065Z"
    }
   },
   "outputs": [],
   "source": [
    "# repeat this process until you get a good set of features\n",
    "all_features = pd.Series(X_train.columns[1:])\n",
    "features = []\n",
    "model2 = None\n",
    "cv_max = 0.0\n",
    "for i in range(1,1000):\n",
    "    featuresset = list(all_features.sample(50)) + tokens_my\n",
    "    new_model, new_cv = build_evaluate_model(X_train[featuresset], y_train, print_stats=False)\n",
    "    if new_cv > cv_max:\n",
    "        cv_max = new_cv\n",
    "        features = featuresset\n",
    "        model2 = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:35:04.852331Z",
     "start_time": "2018-12-05T03:35:04.805375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 700 points : 156\n",
      "Accuracy: 77.71428571428571%\n",
      "[0.76595745 0.80714286 0.78571429 0.72142857 0.82014388]\n",
      "cv average is = 78.01%\n",
      "[[323  31]\n",
      " [125 221]]\n",
      "True Positive = 22.10%\n",
      "False Positive = 3.10%\n",
      "True Negative = 32.30%\n",
      "False Negative = 12.50%\n",
      "sensitivity or hit rate is 63.87%\n",
      "specificity or True Negative rate is 91.24%\n"
     ]
    }
   ],
   "source": [
    "model2, cv2 = build_evaluate_model(X_train[features], y_train, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:35:38.891749Z",
     "start_time": "2018-12-05T03:35:38.878813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 300 points : 72\n",
      "Accuracy: 76.0%\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how it performs on the test set!\n",
    "y_pred2 = model2.predict(X_test[features])\n",
    "\n",
    "n = 300\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        n,\n",
    "        (y_test != y_pred2).sum()\n",
    "    ))\n",
    "correct = (n - (y_test != y_pred2).sum())/n * 100\n",
    "print(\"Accuracy: {}%\".format(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled from the entire set of tokens (50 tokens each) 1000 times, built a Naive Bayes model for each set of features and got our best performing set of features (k = 50). This model was **78.01**% accurate on the train set, and **76%** on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "Let's manually remove some features from model 2 that do not contribute a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:41:39.716322Z",
     "start_time": "2018-12-05T03:41:39.689325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point</th>\n",
       "      <th>buyers</th>\n",
       "      <th>sound-wise</th>\n",
       "      <th>handsfree</th>\n",
       "      <th>explain</th>\n",
       "      <th>NUMCHAR</th>\n",
       "      <th>unintelligible</th>\n",
       "      <th>compromise</th>\n",
       "      <th>hot</th>\n",
       "      <th>cutouts</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "      <th>money</th>\n",
       "      <th>never</th>\n",
       "      <th>not</th>\n",
       "      <th>recommend</th>\n",
       "      <th>very</th>\n",
       "      <th>well</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062147</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.225989</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.031073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182081</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.176301</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          point    buyers  sound-wise  handsfree   explain  NUMCHAR  \\\n",
       "score                                                                 \n",
       "0      0.062147  0.002825    0.002825    0.00000  0.002825      0.0   \n",
       "1      0.000000  0.000000    0.000000    0.00578  0.000000      0.0   \n",
       "\n",
       "       unintelligible  compromise      hot  cutouts    ...        great  \\\n",
       "score                                                  ...                \n",
       "0                 0.0     0.00000  0.00565      0.0    ...     0.005650   \n",
       "1                 0.0     0.00289  0.00289      0.0    ...     0.182081   \n",
       "\n",
       "          happy      love     money     never       not  recommend      very  \\\n",
       "score                                                                          \n",
       "0      0.008475  0.000000  0.036723  0.011299  0.225989   0.005650  0.081921   \n",
       "1      0.028902  0.043353  0.002890  0.005780  0.034682   0.043353  0.176301   \n",
       "\n",
       "           well     worst  \n",
       "score                      \n",
       "0      0.011299  0.031073  \n",
       "1      0.072254  0.000000  \n",
       "\n",
       "[2 rows x 67 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = X_train.copy()\n",
    "x3[\"score\"] = y_train\n",
    "means = x3.groupby(\"score\")[features].mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:51:53.116367Z",
     "start_time": "2018-12-05T03:51:53.109981Z"
    }
   },
   "outputs": [],
   "source": [
    "ratios = (means.iloc[0,:] / means.iloc[1,:]).sort_values()\n",
    "mask = (means.iloc[0,:]==0) & (means.iloc[1,:]==0)\n",
    "ign1 = set(means.columns[mask])\n",
    "ign2 = set(means.columns[ratios.between(0.6,1.4)])\n",
    "ign3 = set(means.columns[(ratios>0) & (ratios<0.3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:53:23.053463Z",
     "start_time": "2018-12-05T03:53:23.049840Z"
    }
   },
   "outputs": [],
   "source": [
    "features3 = list(set(means.columns).difference(ign1).difference(ign2).difference(ign3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:53:59.198091Z",
     "start_time": "2018-12-05T03:53:59.155400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 700 points : 159\n",
      "Accuracy: 77.28571428571429%\n",
      "[0.76595745 0.80714286 0.77857143 0.72142857 0.8057554 ]\n",
      "cv average is = 77.58%\n",
      "[[326  28]\n",
      " [131 215]]\n",
      "True Positive = 21.50%\n",
      "False Positive = 2.80%\n",
      "True Negative = 32.60%\n",
      "False Negative = 13.10%\n",
      "sensitivity or hit rate is 62.14%\n",
      "specificity or True Negative rate is 92.09%\n"
     ]
    }
   ],
   "source": [
    "model3, cv3 = build_evaluate_model(X_train[features3], y_train, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T03:55:26.819785Z",
     "start_time": "2018-12-05T03:55:26.811499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 300 points : 74\n",
      "Accuracy: 75.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how it performs on the test set!\n",
    "y_pred3 = model3.predict(X_test[features3])\n",
    "\n",
    "n = 300\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        n,\n",
    "        (y_test != y_pred3).sum()\n",
    "    ))\n",
    "correct = (n - (y_test != y_pred3).sum())/n * 100\n",
    "print(\"Accuracy: {}%\".format(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T02:25:24.756479Z",
     "start_time": "2018-12-05T02:25:24.747535Z"
    }
   },
   "source": [
    "So this model's performance was slightly worse, at **77.58%** on the train set and **75.33%** on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T00:22:42.086184Z",
     "start_time": "2018-12-05T00:22:42.032716Z"
    }
   },
   "source": [
    "## Model 4\n",
    "We will oversample our negative tokens for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:02:40.171322Z",
     "start_time": "2018-12-05T04:02:08.201696Z"
    }
   },
   "outputs": [],
   "source": [
    "# repeat this process until you get a good set of features\n",
    "features4 = []\n",
    "model4 = None\n",
    "cv_max = 0.0\n",
    "for i in range(1,1000):\n",
    "    featuresset = random.sample(tokens_negative, 35) + random.sample(tokens_positive, 15) + tokens_my\n",
    "    new_model, new_cv = build_evaluate_model(X_train[featuresset], y_train, print_stats=False)\n",
    "    if new_cv > cv_max:\n",
    "        cv_max = new_cv\n",
    "        features4 = featuresset\n",
    "        model4 = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:02:45.076229Z",
     "start_time": "2018-12-05T04:02:45.031749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 700 points : 155\n",
      "Accuracy: 77.85714285714286%\n",
      "[0.78014184 0.77857143 0.79285714 0.71428571 0.82014388]\n",
      "cv average is = 77.72%\n",
      "[[328  26]\n",
      " [129 217]]\n",
      "True Positive = 21.70%\n",
      "False Positive = 2.60%\n",
      "True Negative = 32.80%\n",
      "False Negative = 12.90%\n",
      "sensitivity or hit rate is 62.72%\n",
      "specificity or True Negative rate is 92.66%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " 0.7772000029156005)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_evaluate_model(X_train[features4], y_train, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:03:00.296262Z",
     "start_time": "2018-12-05T04:03:00.287849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 300 points : 76\n",
      "Accuracy: 74.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how it performs on the test set!\n",
    "y_pred4 = model4.predict(X_test[features4])\n",
    "\n",
    "n = 300\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        n,\n",
    "        (y_test != y_pred4).sum()\n",
    "    ))\n",
    "correct = (n - (y_test != y_pred4).sum())/n * 100\n",
    "print(\"Accuracy: {}%\".format(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T23:47:44.448739Z",
     "start_time": "2018-12-04T23:47:44.422975Z"
    }
   },
   "source": [
    "So when we oversample negative tokens, our model is slightly more accurate on the train set at **77.85%** but less on the test set at **74.66%**, however the specificity (true negative rate) increases (as expected), but not by very much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5\n",
    "\n",
    "*VADER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:05:33.461225Z",
     "start_time": "2018-12-05T04:05:33.455807Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def nltk_sentiment(sentence, return_list = True):\n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    # score will look like:\n",
    "    # {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "    if return_list:\n",
    "        # [0.0, 0.667, 0.333, 0.3612]\n",
    "        score = [ item[1] for item in score.items() ]\n",
    "        return score[0], score[1], score[2], score[3]\n",
    "    else:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:09:06.484673Z",
     "start_time": "2018-12-05T04:09:00.572115Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = X_train.review_processed.apply(nltk_sentiment)\n",
    "X_train[\"neg\"] = [score[0] for score in scores]\n",
    "X_train[\"neu\"] = [score[1] for score in scores]\n",
    "X_train[\"pos\"] = [score[2] for score in scores]\n",
    "X_train[\"compound\"] = [score[3] for score in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:10:18.133414Z",
     "start_time": "2018-12-05T04:10:18.094574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 700 points : 112\n",
      "Accuracy: 84.0%\n",
      "[0.87234043 0.81428571 0.85       0.82857143 0.83453237]\n",
      "cv average is = 83.99%\n",
      "[[297  57]\n",
      " [ 55 291]]\n",
      "True Positive = 29.10%\n",
      "False Positive = 5.70%\n",
      "True Negative = 29.70%\n",
      "False Negative = 5.50%\n",
      "sensitivity or hit rate is 84.10%\n",
      "specificity or True Negative rate is 83.90%\n"
     ]
    }
   ],
   "source": [
    "features5 = [\"neg\",\"neu\",\"pos\",\"compound\"]\n",
    "model5, cv5 = build_evaluate_model(X_train[features5], y_train, print_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:11:06.546947Z",
     "start_time": "2018-12-05T04:11:03.929368Z"
    }
   },
   "outputs": [],
   "source": [
    "scores2 = X_test.review_processed.apply(nltk_sentiment)\n",
    "X_test[\"neg\"] = [score[0] for score in scores2]\n",
    "X_test[\"neu\"] = [score[1] for score in scores2]\n",
    "X_test[\"pos\"] = [score[2] for score in scores2]\n",
    "X_test[\"compound\"] = [score[3] for score in scores2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:12:27.286983Z",
     "start_time": "2018-12-05T04:12:27.276440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 300 points : 47\n",
      "Accuracy: 84.33333333333334%\n"
     ]
    }
   ],
   "source": [
    "# Now let's see how it performs on the test set!\n",
    "y_pred5 = model5.predict(X_test[features5])\n",
    "\n",
    "n = 300\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "        n,\n",
    "        (y_test != y_pred5).sum()\n",
    "    ))\n",
    "correct = (n - (y_test != y_pred5).sum())/n * 100\n",
    "print(\"Accuracy: {}%\".format(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the VADER algorithm to give the review a negative, neutral, positive and compound scores, and then uses these 4 features to predict the sentiment. It was **83.99%** accurate on the train set, and **84.33%** accurate on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
