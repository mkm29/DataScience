# Challenge 3.5.5

You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. To make sure all of these ideas are organized in your mind, please go through the list of problems below. For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor.

1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.

   Answer - This looks like a good time to use linear regression. You first may want to perform feature selection via something like ridge or lasso regression, or perform PCA on the input features.

2. You have more features (columns, m) than rows (n) in your dataset.

   Answer: This indicates you first need to reduce the dimensions of your data; for this use Ridge/Lasso regression and select the top k largest features (according to coeffecient value), or PCA the features and take the k most components (according to variance explained).

3. Identify the most important characteristic predicting likelihood of being jailed before age 20.

   Answer: Random forests, ridge and lasso regression and gradient boosting all provide feature importance information.

4. Implement a filter to “highlight” emails that might be important to the recipient.

   Answer: The simplest way would be to use a Naive Bayes model; this is a classification problem so most models can also be classifiers (logistic regression, ridge regression, random forests).

5. You have 1000+ features.

   Answer: SVM's perform relatively well on large feature sets. However, feature reduction might not hurt so lasso/ridge regression or random forests can identify which features to keep. Gradient boosting can as well. 

6. Predict whether someone who adds items to their cart on a website will purchase the items.

   Answer: First we might want to engineer some features via transformations and/or interactions, and then we can feed these to a model to perform classification. Once again, most models that we have covered thus far can be adapted to perform classification, however a random forest seems the most practical here.

7. Your dataset dimensions are 982400 x 500.

   Answer: What is your objective? What kind of data are we talking about? You may want to reduce the feature set, and due to the large number of examples a SVM sounds reasonable here. The next step in our pipeline will be to feed this data to a model, of which the problem objective will dictate the model type. 

8. Identify faces in an image.

   Answer: Typically, a CNN is used to identify facial features. This type of neural network can be implemented by hand via Numpy. If we cannot use deep learning, it may be possible to convert a 3 channel image (RGB) into monochrome, then "roll" up this n x m vector into an (m x n) column vector in which we could then perform PCA on it, associating certain facial features with different components. We can then aggregate these features and feed them to a logistic regression classifier.

9. Predict which of three flavors of ice cream will be most popular with boys vs girls.

   Answer: A random forest seems applicable here. (Easy to interpret, fast to train).