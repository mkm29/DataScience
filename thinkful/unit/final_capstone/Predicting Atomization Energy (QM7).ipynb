{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T13:55:27.308610Z",
     "start_time": "2019-04-21T13:55:23.451503Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import time\n",
    "rand_state = 42\n",
    "tf.set_random_seed(rand_state)\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline  \n",
    "\n",
    "1. Introduction\n",
    "2. Data\n",
    "  * Feature Engineering\n",
    "3. Visualization\n",
    "  * Kernel PCA\n",
    "  * t-SNE\n",
    "4. Classification\n",
    "  * SVM\n",
    "  * Classification metrics\n",
    "5. Regression\n",
    "  * XGBoost\n",
    "  * Deep Learning\n",
    "    * MLP\n",
    "    * CNN\n",
    "6. Conclusions\n",
    "\n",
    "\n",
    "\n",
    "# Introduction  \n",
    "\n",
    "Research and development (R&D) costs for US based pharmaceutical companies have risen 43.9% just over the last 5 years ([source](https://www.statista.com/statistics/265085/research-and-development-expenditure-us-pharmaceutical-industry/)). R&D spending has increased by 27% since 2010 and is projected to increase by another 23.6% over the next 5 years; R&D accounts for roughly 21% of all pharmaceutical sales, this pipeline can be summarized as so:  \n",
    "\n",
    "  1. Pre-clinical  \n",
    "  2. Phase 1 clinical trials  \n",
    "  3. Phase II  \n",
    "  4. Phase III  \n",
    "  5. Pre-registration  \n",
    "  6. Registered  \n",
    "  7. Launched  \n",
    "  8. Suspended  \n",
    "  \n",
    "The most resources (by far) are spent in the pre-clinical stage ([source](https://www.statista.com/statistics/791288/r-and-d-pipeline-drugs-worldwide-by-phase-development/)), with drug discovery being one of the initial steps in this process. Drug discovery is the process by which new candidate medications are discovered. In classical pharmacology, chemical libraries of synthetic small molecules, natural products or extracts are utilized in combination with in-vitro studies to identify substances that have a desirable therapeutic effect. \n",
    "\n",
    "Since the sequencing of the human genome, it has become common practice to use high throughput screening of large compounds libraries against isolated biological targets which are hypothesized to be disease modifying in a process known as reverse pharmacology. The candidate drugs are then tested for efficacy in-vivo. Various chemical properties, such as ADME (absorption, distribution, metabolism and excretion) as well as atomization energy have been shown to play a crucial role in the screening of such compounds, and that is where machine learning can help! The accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design.  \n",
    "\n",
    "Currently only high-level quantum-chemistry calculations, which can take days per molecule depending on property and system, yield the desired “chemical\n",
    "accuracy” of 1 kcal/mol required for computational molecular design. Here, I am proposing a less expensive machine learning approach to predicting this the quantum mechanical property of atomization energy. To accomplish this objective, I will engineer features to work well with a gradient boosting regression model, support vector machine classification model, as well as a multilayer perceptron network and a convolutional neural network. \n",
    "\n",
    "# Data  \n",
    "\n",
    "I am using the QM7 dataset, which is a subset of GDB-13 (a database of nearly 1 billion stable and synthetically accessible organic molecules) containing up to 7 heavy atoms C, N, O, and S. The 3D Cartesian coordinates of the most stable conformations and their atomization energies were determined using ab-initio density functional theory (PBE0/tier2 basis set). This dataset also provided Coulomb matrices as calculated in [Rupp et al. PRL, 2012]:  \n",
    "  * $C_{i,i} = 0.5 \\cdot Z^2.4$  \n",
    "  * $C_{i,j} = Z_i \\cdot \\frac{Z_j}{|(R_i−R_j)|}$ \n",
    "  * $Z_i$ - nuclear charge of atom i  \n",
    "  * $R_i$ - cartesian coordinates of atom i  \n",
    "\n",
    "The data file (.mat format, we recommend using `scipy.io.loadmat` for python users) contains five arrays:  \n",
    "  * \"X\" - (7165 x 23 x 23), Coulomb matrices  \n",
    "  * \"T\" - (7165), atomization energies (unit: kcal/mol)  \n",
    "  * \"P\" - (5 x 1433), cross-validation splits as used in [Montavon et al. NIPS, 2012]  \n",
    "  * \"Z\" - (7165 x 23), atomic charges  \n",
    "  * \"R\" - (7165 x 23 x 3), cartesian coordinate (unit: Bohr) of each atom in the molecules   \n",
    "\n",
    "\n",
    "## Feature Engineering  \n",
    "\n",
    "The data is stored in a MATLAB file, so we read that using SciPy. For the regression models, I take the upper triangle of each Coulomb Matrix (with diagonal) and unroll it so each has the shape: (1, num_atoms*(num_atoms+1)/2). I then compute the pairwise distance matrix and from this get the corresponding Eigenvalues and [Eigenvector Centralities](https://en.wikipedia.org/wiki/Eigenvector_centrality). I am attempting to estimate the influence one particular atom plays on the entire molecule; typically, this is performed on an adjacency matrix (graph), however, I am assuming the connectivity (bonds) are not known upfront (can be accurately predicted via Monte Carlo simulations).  \n",
    "\n",
    "## Previous work  \n",
    "This is an extension to some earlier research that I previously [conducted](https://www.kaggle.com/mjmurphy28/predicting-ground-state-energy). That study (and therefore this one also) is primarily based off two previous studies: a 2016 publication by [B. Himmetoglu](https://arxiv.org/abs/1609.07124) and a team of researchers at [Stanford](http://moleculenet.ai). The earlier work by Himmetoglu reported a RMSE of 38.75 kcal/mol on the train set and 36.83 on the test set. I was not able to find the full results from the team at Stanford, however, on their [site](http://moleculenet.ai/latest-results) they list a MAE of 8.56 (I am assuming that this metric comes from evaluating their TensorFlow regression model on the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T13:55:42.167007Z",
     "start_time": "2019-04-21T13:55:41.144603Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "qm7 = scipy.io.loadmat('./data/qm7/qm7.mat')\n",
    "X_cm = qm7['X']\n",
    "# compute the Eigenvectors of the pairwise distance matrix?\n",
    "R = qm7['R']\n",
    "Z = qm7['Z']\n",
    "y = np.transpose(qm7['T']).reshape((7165,))\n",
    "\n",
    "\n",
    "num_atoms = X_cm.shape[1]\n",
    "X_eigs = np.zeros((X_cm.shape[0], num_atoms, num_atoms), dtype=float)\n",
    "\n",
    "for i, cm in enumerate(X_cm):\n",
    "    w, v = np.linalg.eig(cm)\n",
    "    X_eigs[i] = v\n",
    "\n",
    "\n",
    "y_scaling_factor = 2000.\n",
    "y_scaled = y / y_scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also computed the number of atoms per molecule, primarily for visualization purposes. This data presents an interesting problem, as the number of atoms per molecule is not fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T13:57:18.197165Z",
     "start_time": "2019-04-21T13:56:21.341593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7165, 576)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=0 # 0 = include diagnol, 1 = do not include diagnol\n",
    "\n",
    "iu = np.triu_indices(num_atoms,k=k) \n",
    "iu_dist = np.triu_indices(num_atoms,k=1) # for the pairwise distance matrix, all diagonol entries will be 0 \n",
    "\n",
    "\n",
    "\n",
    "data_atoms = np.zeros((X_cm.shape[0], 1), dtype=int)\n",
    "data_CM = np.zeros((X_cm.shape[0], num_atoms*(num_atoms+1)//2), dtype=float)\n",
    "data_eigs = np.zeros((X_cm.shape[0], num_atoms), dtype=float)\n",
    "data_centralities = np.zeros((X_cm.shape[0], num_atoms), dtype=float)\n",
    "data_pdist = np.zeros((X_cm.shape[0], ((num_atoms*num_atoms)-num_atoms)//2), dtype=float) \n",
    "\n",
    "for i, cm in enumerate(X_cm):\n",
    "    coulomb_vector = cm[iu]\n",
    "    # Sort elements by decreasing order\n",
    "    shuffle = np.argsort(-coulomb_vector)\n",
    "    data_CM[i] = coulomb_vector[shuffle]\n",
    "    data_atoms[i] = np.count_nonzero(R[i], axis=0)[0]\n",
    "    dist = squareform(pdist(R[i]))\n",
    "    # we can extract the upper triangle of the distance matri: return vector of dimension (1,num_atoms)\n",
    "    dist_vector = dist[iu_dist]\n",
    "    shuffle = np.argsort(-dist_vector)\n",
    "    data_pdist[i] = dist_vector[shuffle]\n",
    "    \n",
    "    w,v = np.linalg.eig((dist))\n",
    "    data_eigs[i] = w[np.argsort(-w)]\n",
    "    data_centralities[i] = np.array(list(nx.eigenvector_centrality(nx.Graph(dist)).values()))\n",
    "    \n",
    "X = np.concatenate((data_CM, data_eigs, data_centralities, data_pdist, data_atoms), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Molecular Graph**  \n",
    "\n",
    "Let's see if we can make a graph from simply the 3 dimensional coordinates (remember we are acting as if we do not know the bonds). We can then use this graph to compute various properties such as the connectivity of nodes (atoms).  \n",
    "\n",
    "I am going to use [van der Waal radii](https://en.wikipedia.org/wiki/Atomic_radii_of_the_elements_(data_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T15:10:42.077544Z",
     "start_time": "2019-04-21T15:10:42.073683Z"
    }
   },
   "outputs": [],
   "source": [
    "# these radii measurements are in picometers: 10^-12 m\n",
    "\n",
    "radii = {\n",
    "    1: 120e-12,\n",
    "    6: 170e-12,\n",
    "    7: 155e-12,\n",
    "    8: 152e-12,\n",
    "    9: 147e-12, \n",
    "    14: 210e-12,\n",
    "    15: 180e-12,\n",
    "    16: 180e-12,\n",
    "    17: 175e-12\n",
    "}\n",
    "\n",
    "valence_elctrons = {\n",
    "    1: 1,\n",
    "    6: 4,\n",
    "    7: 5,\n",
    "    8: 6,\n",
    "    9: 7,\n",
    "    14: 4,\n",
    "    15: 5,\n",
    "    16: 6,\n",
    "    17: 7 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T14:19:01.242692Z",
     "start_time": "2019-04-21T14:19:01.238138Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Create Node\n",
    "    -----------\n",
    "    1. Create position vector\n",
    "    2. \n",
    "\n",
    "'''\n",
    "def pos_for_molecule(molecule_coords):\n",
    "    # how many nodes in this molecule?\n",
    "    num_nodes = np.count_nonzero(molecule_coords, axis=1)[0]+1\n",
    "    pos = {}\n",
    "    #pos = {i: (random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)) for i in range(n_nodes)}\n",
    "    for i, coords in enumerate(molecule_coords):\n",
    "        if i > num_nodes:\n",
    "            break\n",
    "        pos[i] = (coords[0], coords[1], coords[2])\n",
    "    return num_nodes, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T15:43:35.815979Z",
     "start_time": "2019-04-21T15:43:35.735899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mol_idx = 0\n",
    "\n",
    "num_nodes, positions = pos_for_molecule(R[mol_idx])\n",
    "\n",
    "G = nx.Graph()\n",
    "pos = dict()\n",
    "for i, couple in enumerate(positions):\n",
    "        G.add_node(i)\n",
    "        pos[i] = couple\n",
    "\n",
    "\n",
    "# now make up radius for each node\n",
    "# this should be the Bohr Radius corresponding to the type of atom (hydrogen)\n",
    "for i, node in enumerate(G.nodes()):\n",
    "    G.node[node]['rad'] = radii.get(Z[mol_idx][i], 0.0)\n",
    "\n",
    "nx.draw_circular(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T16:08:46.140227Z",
     "start_time": "2019-04-21T16:08:46.133175Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Create most likely bond. To compute this we need to:\n",
    "        1. Compute pairwise distance between all nodes\n",
    "        2. \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "    need to adjust this matrix to account for the radius of each atom\n",
    "    going to add the van der Waal radius to the node in question\n",
    "    and going to subtract the radius from all other nodes in the network\n",
    "''' \n",
    "\n",
    "def get_bonds(mol_idx):\n",
    "    dm = squareform(data_pdist[mol_idx])[:num_nodes, :num_nodes]\n",
    "    dm2 = dm.copy()\n",
    "\n",
    "    for i, j in zip(range(dm.shape[0]), range(dm.shape[1])):\n",
    "        if i == j:\n",
    "            continue\n",
    "        r1 = radii.get(Z[0][i])\n",
    "        r2 = radii.get(Z[0][j])\n",
    "        # now update\n",
    "        dm2[i,j] = dm[i,j] - r1 - r2\n",
    "\n",
    "    bonds = np.zeros((dm2.shape))\n",
    "\n",
    "    for i, j in zip(range(dm2.shape[0]), range(dm2.shape[1])):\n",
    "        # these are the possible nodes node i can bind to\n",
    "        binding = np.delete(np.argsort(dm2[i,:]), [i])[:valence_elctrons.get(Z[0][i])]\n",
    "        # create an edge for all of the above (just set to 1)\n",
    "        # and set i,j to 0\n",
    "        bonds[i, (i+1):] = binding>0\n",
    "\n",
    "    return bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T16:08:52.152825Z",
     "start_time": "2019-04-21T16:08:52.149902Z"
    }
   },
   "outputs": [],
   "source": [
    "bonds = get_bonds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "\n",
    "Do I need to **_reshape_** these adjacency matrices? \n",
    "\n",
    "For example, a molecule that only contains 5 atoms (so max of 4 bonds) will be a 5 by 5 adjacency matrix, padded with zeros to all adj. matrices are of the shape (23, 23)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**  \n",
    "\n",
    "Because the values of the Coulomb Matrices can vary so widely (0 to 388.02344) we will need to scale/normalize these values for input to any deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "CM_norm = X_cm.copy()\n",
    "\n",
    "for i,cm in enumerate(X_cm):\n",
    "    CM_norm[i] = normalize(cm, axis=1, norm='l1')\n",
    "    \n",
    "CM_norm = CM_norm.reshape((7165, 23, 23, 1))\n",
    "\n",
    "X_eigs = X_eigs.reshape((7165, 23, 23, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_dist(x):\n",
    "    x[x == 0] = np.nan\n",
    "    return np.nanmean(x, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "mean_dists = np.apply_along_axis(mean_dist, axis=1, arr=data_pdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Histogram of number of atoms')\n",
    "plt.hist(data_atoms, bins=20)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data_atoms.reshape(-1,), y)\n",
    "plt.xlabel('Number of atoms')\n",
    "plt.ylabel('E (kcal/mol)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(mean_dists)\n",
    "plt.xlabel('Interatomic Distance')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of interatomic distances')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(y)\n",
    "plt.xlabel('Energy (kcal/mol)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of Atomization Energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualize just the **Coulomb Matrices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from matplotlib import cm\n",
    "\n",
    "start_time = time.time()\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\")\n",
    "CM_reduced = kpca.fit_transform(data_CM)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "explained_variance = np.var(CM_reduced, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "print(\"Variance Explained: \", np.sum(explained_variance_ratio))\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "scatter = ax.scatter(CM_reduced[:,0], CM_reduced[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\n",
    "colorbar = fig.colorbar(scatter, ax=ax, label = \"(kcal/mol)\")\n",
    "plt.xlabel(r'$k-PCA_1$')\n",
    "plt.ylabel(r'$k-PCA_1$')\n",
    "plt.title(\"Visualizing Coulomb Matrix: RBF Kernel\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add all features (576)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "kpca = KernelPCA(n_components=2, kernel=\"cosine\")\n",
    "X_reduced = kpca.fit_transform(X)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "explained_variance = np.var(X_reduced, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "print(\"Variance Explained: \", np.sum(explained_variance_ratio))\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "scatter = ax.scatter(X_reduced[:,0], X_reduced[:,1], c=y, s=60, edgecolors='black', cmap=cm.jet_r)\n",
    "colorbar = fig.colorbar(scatter, ax=ax, label = \"E (kcal/mol)\")\n",
    "plt.xlabel(r'$k-PCA_1$')\n",
    "plt.ylabel(r'$k-PCA_1$')\n",
    "plt.title(\"Visualizing All Features: Cosine Kernel\")\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into (roughly) equal groups\n",
    "              \n",
    "def get_category(x):\n",
    "    ''' > -1700, (-1700, -1540), (-1540, -1370), > -1370 '''\n",
    "    if x < -1700:\n",
    "        return 0\n",
    "    elif -1700 <= x < -1540:\n",
    "        return 1\n",
    "    elif -1540 <= x < -1370:\n",
    "        return 2\n",
    "    elif x > -1370:\n",
    "        return 3\n",
    "\n",
    "y_class = pd.Series(y).apply(get_category).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification  \n",
    "\n",
    "First I will create a model to classify a given atom into one of four classes, based on atomization energy. I have manually constructed my classes so that they are roughly balanced. A tool like this can provide us insight into 'tricky' molecules, those that are difficult to distinguish among.  \n",
    "\n",
    "I will split my data into **70/20/10** train/dev/val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_2, X_val, y_class_2, y_class_val = train_test_split(X, y_class.astype(int), \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=rand_state)\n",
    "\n",
    "# with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "X_train, X_dev, y_class_train, y_class_dev = train_test_split(X_2, y_class_2, \n",
    "                                                    test_size=0.18, \n",
    "                                                    random_state=rand_state)\n",
    "\n",
    "clf = SVC(kernel='linear', random_state=rand_state)\n",
    "clf.fit(X_train, y_class_train)\n",
    "print(\"Train score: \", clf.score(X_train, y_class_train))\n",
    "print(\"Test score: \", clf.score(X_dev, y_class_dev))\n",
    "\n",
    "print('--------\\nEVALUATE on validation set\\n--------')\n",
    "class_preds = clf.predict(X_val)\n",
    "print(classification_report(y_class_val, class_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassificationReport, ClassPredictionError\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassificationReport(SVC(kernel='linear', random_state=rand_state), classes=[0,1,2,3], support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_class_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_val, y_class_val)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassPredictionError(\n",
    "    SVC(kernel='linear', random_state=rand_state), classes=[0,1,2,3]\n",
    ")\n",
    "\n",
    "# X_train, X_test, y_class_train, y_class_test\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_class_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_val, y_class_val)\n",
    "\n",
    "# Draw visualization\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(SVC(kernel='linear', random_state=rand_state), classes=[0,1,2,3])\n",
    "\n",
    "visualizer.fit(X_train, y_class_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_val, y_class_val)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "\n",
    "# Create the visualizer, fit, score, and poof it\n",
    "viz = PrecisionRecallCurve(SVC(kernel='linear', random_state=rand_state), classes=[0,1,2,3])\n",
    "viz.fit(X_train, y_class_train)\n",
    "viz.score(X_val, y_class_val)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusons**  \n",
    "\n",
    "I was able to build a pretty good classification model; one that predicts the most likely bin or range of atomization energies that molecule belongs to. Our intervals were as follows:\n",
    "  1. y < -1700\n",
    "  2. -1700 <= y < -1540\n",
    "  3. -1540 <= y < -1370\n",
    "  4. y > -1370\n",
    "\n",
    "We see high AUC values for all classes, except for class 2. This might indicate that detecting patterns with explanatory power is more difficult for molecules belonging to that class. In other words, we would expect to be more confident in predicting atomization energies less than -1700 kcal/mol over those exhibiting energies above -1540. While the interatomic distances offered some explanatory power for classification purposes, they aactually cause the regression model to be less accurate so we will drop these features for the regression portion of this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "First, I am going to split my data into 70/15/15 train/dev/validation sets, and use 3-fold cross validation when training my model. Through experimentation (as well as previous [work](https://www.kaggle.com/mjmurphy28/predicting-ground-state-energy)) XGBoost provides excellent accuracy, while very low training time. I settled on the following hyper-parameters through intuition, literature as well as trial and error. As mentioned above, a number of the features that I engineered  solely for classification purposes do not add any explanatory power to my regression model, therefore I will only be using the unrolled Coulomb Matrix, Eigenvector centralities and Eigenvalues (of the interatomic distance matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X = np.concatenate((data_CM, data_eigs, data_centralities), axis=1)\n",
    "\n",
    "\n",
    "# with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "X_2, X_val, y_2, y_val = train_test_split(X, y, \n",
    "                                          test_size=0.15, \n",
    "                                          random_state=rand_state)\n",
    "\n",
    "# with cross validation no need to further split the data. if not using cross validation you should do one...\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_2, y_2, \n",
    "                                                  test_size=0.18, \n",
    "                                                  random_state=rand_state)\n",
    "\n",
    "\n",
    "n_folds = 5\n",
    "early_stopping = 50\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "params = {\"objective\":\"reg:linear\", \n",
    "          'booster': 'gbtree', \n",
    "          'eval_metric': 'mae',\n",
    "          'subsample': 0.9,\n",
    "          'colsample_bytree':0.2,\n",
    "          'learning_rate': 0.06,\n",
    "          'max_depth': 5, \n",
    "          'reg_lambda': .9, \n",
    "          'reg_alpha': .01,\n",
    "          'seed': rand_state}\n",
    "\n",
    "\n",
    "cv = xgb.cv(params,\n",
    "            xg_train, \n",
    "            num_boost_round=200, \n",
    "            nfold=n_folds, \n",
    "            early_stopping_rounds=early_stopping, \n",
    "            verbose_eval = 0, \n",
    "            seed=rand_state,\n",
    "            as_pandas=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(cv['train-mae-mean'][100:], label='Train loss: ' + str(np.min(cv['train-mae-mean'])))\n",
    "plt.plot(cv['test-mae-mean'][100:], label='Test loss: ' + str(np.min(cv['test-mae-mean'])))\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean absolute error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding regularization I was able to decrease the amount our model overfits to the train data. Note that if you run this model for more boosting rounds (iterations) the model begins to significantly overfit. Setting the max depth of each tree also helps reduce this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(**params, random_state=rand_state, n_estimators=200)\n",
    "model_xgb.fit(X_train, y_train, \n",
    "              early_stopping_rounds=early_stopping, \n",
    "              eval_metric='mae', \n",
    "              eval_set=[(X_dev, y_dev)], \n",
    "              verbose=False)\n",
    "\n",
    "y_dev_pred = model_xgb.predict(X_dev)\n",
    "print('Dev mean absoulte error: ', mean_absolute_error(y_dev, y_dev_pred))\n",
    "\n",
    "y_val_pred = model_xgb.predict(X_val)\n",
    "print('Validation mean absoulte error: ', mean_absolute_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "visualizer = PredictionError(xgb.XGBRegressor(**params, n_estimators=200, random_state=rand_state))\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_val, y_val)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of the distributions, notably those belonging to class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "visualizer = ResidualsPlot(xgb.XGBRegressor(**params, n_estimators=200, random_state=rand_state))\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_val, y_val)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "\n",
    "## Feedforward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, PReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_unrolled = CM_norm.reshape((7165, 529))\n",
    "X_cm_train, X_cm_test, y_train, y_test  = train_test_split(CM_unrolled, y_scaled, test_size=.2, random_state=rand_state)\n",
    "\n",
    "\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_cm_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "#NN_model.add(Dropout(.2))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "#NN_model.add(Dropout(.2))\n",
    "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "#NN_model.add(Dropout(.2))\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', \n",
    "                 optimizer=optimizers.Adam(lr=.0001), \n",
    "                 metrics=['mean_absolute_error'])\n",
    "NN_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "NN_model.fit(X_cm_train, y_train, \n",
    "             validation_data=(X_cm_test, y_test), \n",
    "             verbose=0,\n",
    "             epochs=50, \n",
    "             batch_size=8, \n",
    "             callbacks=[EarlyStopping(monitor='val_loss', patience=10)])\n",
    "\n",
    "print('Train loss: ', 2000*mean_absolute_error(y_train, NN_model.predict(X_cm_train)))\n",
    "print('Test loss: ', 2000*mean_absolute_error(y_test, NN_model.predict(X_cm_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network \n",
    "\n",
    "Translational invariance is a critical aspect to why convnet's are so effective when dealing with images, and here I am envisioning the Coulomb Matrix similar to a single-channel image (matrix of pixels), and I suspect that the Coulomb Matrix exhibits some form of translational invariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "convnets require specific input dimensions, in this case every example should be a (23, 23, 1) Coulom Matrix\n",
    "\n",
    "'''\n",
    "X_cm_train, X_cm_test, y_train, y_test  = train_test_split(CM_norm, y_scaled, test_size=.2, random_state=rand_state)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5,5), padding='same',\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(23, 23, 1)))\n",
    "''' \n",
    "    (23, 23, 1) => (19, 19, 32)\n",
    "'''\n",
    "#model.add(MaxPooling2D((2,1), strides=(2,1)))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "'''\n",
    "    (23, 23, 1) => (19, 19, 32) => (17, 17, 32)\n",
    "'''\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "'''\n",
    "    (23, 23, 1) => (19, 19, 32) => (17, 17, 32) => (15, 15, 64)\n",
    "'''\n",
    "\n",
    "'''\n",
    "1x1 convolutions: https://iamaaditya.github.io/2016/03/one-by-one-convolution/\n",
    "'''\n",
    "# mixing of channels\n",
    "model.add(Conv2D(64, kernel_size=(1,1), activation=PReLU()))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "'''\n",
    "    (23, 23, 1) => (19, 19, 32) => (17, 17, 32) => (15, 15, 64) => (15, 15, 64) => (7, 7, 64)\n",
    "'''\n",
    "model.add(Flatten())\n",
    "# (3136, 1)\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "# 3136*256 = 803072\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mae',\n",
    "              optimizer=optimizers.Adam(lr=.001),\n",
    "              metrics=['mae'])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_cm_train, y_train,\n",
    "                      batch_size=8,\n",
    "                      epochs=50, \n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=10)],\n",
    "                      verbose=0,\n",
    "                      validation_data=(X_cm_test, y_test))\n",
    "\n",
    "end = time.time()\n",
    "print('Execution time: ', end-start)\n",
    "print(\"Epochs: \", len(history.history['val_loss']))\n",
    "print('Train loss: ', 2000*np.min(history.history['loss']))\n",
    "print('Test loss: ', 2000*np.min(history.history['val_loss']))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion  \n",
    "\n",
    "\n",
    "# References  \n",
    "\n",
    "  * https://papers.nips.cc/paper/4830-learning-invariant-representations-of-molecules-for-atomization-energy-prediction.pdf\n",
    "  * Rupp, Matthias, et al. \"Fast and accurate modeling of molecular atomization energies with machine learning.\" Physical review letters 108.5 (2012): 058301.\n",
    "  * Montavon, Grégoire, et al. \"Learning invariant representations of molecules for atomization energy prediction.\" Advances in Neural Information Processing Systems. 2012. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
