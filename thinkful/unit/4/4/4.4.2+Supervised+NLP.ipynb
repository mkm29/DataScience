{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T03:44:59.717855Z",
     "start_time": "2019-02-13T03:44:59.707917Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:46:48.715260Z",
     "start_time": "2019-02-13T01:46:48.683963Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:47:23.426611Z",
     "start_time": "2019-02-13T01:46:53.482829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:47:26.452441Z",
     "start_time": "2019-02-13T01:47:26.417249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.columns = ['text', 'author']\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:47:35.088752Z",
     "start_time": "2019-02-13T01:47:34.964939Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "    \n",
    "def bag_of_lemma(sentences, common_token_lemmas, verbose=False, batch_size=250):\n",
    "    # use word counts\n",
    "    lemma_counts = pd.DataFrame(columns=common_token_lemmas)\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        # repeat this for every sentence\n",
    "        x = pd.Series(sentence.lemma_.split(' ')).value_counts()\n",
    "        x = x[pd.Series([str(x) for x in x.index]).isin(common_token_lemmas).values]\n",
    "        lemma_counts.loc[i,:] = 0\n",
    "        lemma_counts.loc[i,[str(x) for x in x.index]] = x.values\n",
    "        if verbose:\n",
    "            if i % batch_size == 0:\n",
    "                    print(\"Processing row {}\".format(i))\n",
    "        i += 1\n",
    "    return lemma_counts\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:47:38.187073Z",
     "start_time": "2019-02-13T01:47:38.181694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:47:46.728162Z",
     "start_time": "2019-02-13T01:47:42.264466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert tokens to lemmas\n",
    "lmtzr = nltk.WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(alice)\n",
    "token_lemma = [ lmtzr.lemmatize(token) for token in tokens ]\n",
    "token_lemma_alice = set(token_lemma)\n",
    "\n",
    "lmtzr = nltk.WordNetLemmatizer()\n",
    "tokens = nltk.word_tokenize(persuasion)\n",
    "token_lemma = [ lmtzr.lemmatize(token) for token in tokens ]\n",
    "token_lemma_persuasion = set(token_lemma)\n",
    "common_token_lemmas = token_lemma_alice.union(token_lemma_persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:47:48.355094Z",
     "start_time": "2019-02-13T01:47:48.347986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7146"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_token_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T02:54:00.641668Z",
     "start_time": "2019-02-02T02:54:00.636909Z"
    }
   },
   "source": [
    "**Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:52:06.761690Z",
     "start_time": "2019-02-13T01:52:06.750001Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_bag_of_words(sentences_df, common_words, verbose=False, batch_size=250):\n",
    "    word_counts = pd.DataFrame(columns=common_words)\n",
    "    i = 0\n",
    "    for sentence in sentences_df.iloc[:,0]:\n",
    "        # repeat this for every sentence\n",
    "        x = pd.Series(sentence).value_counts()\n",
    "        x = x[pd.Series([str(x) for x in x.index]).isin(common_words).values]\n",
    "        word_counts.loc[i,:] = 0\n",
    "        word_counts.loc[i,[str(x) for x in x.index]] = x.values\n",
    "        if verbose:\n",
    "            if i % batch_size == 0:\n",
    "                print(\"Processing row {}\".format(i))\n",
    "        i += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:59:15.993848Z",
     "start_time": "2019-02-13T01:52:35.492300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 250\n",
      "Processing row 500\n",
      "Processing row 750\n",
      "Processing row 1000\n",
      "Processing row 1250\n",
      "Processing row 1500\n",
      "Processing row 1750\n",
      "Processing row 2000\n",
      "Processing row 2250\n",
      "Processing row 2500\n",
      "Processing row 2750\n",
      "Processing row 3000\n",
      "Processing row 3250\n",
      "Processing row 3500\n",
      "Processing row 3750\n",
      "Processing row 4000\n",
      "Processing row 4250\n",
      "Processing row 4500\n",
      "Processing row 4750\n",
      "Processing row 5000\n",
      "Processing row 5250\n"
     ]
    }
   ],
   "source": [
    "word_counts = my_bag_of_words(sentences, common_words, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T22:29:39.231432Z",
     "start_time": "2019-02-12T22:19:27.857631Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 300\n",
      "Processing row 600\n",
      "Processing row 900\n",
      "Processing row 1200\n",
      "Processing row 1500\n",
      "Processing row 1800\n",
      "Processing row 2100\n",
      "Processing row 2400\n",
      "Processing row 2700\n",
      "Processing row 3000\n",
      "Processing row 3300\n",
      "Processing row 3600\n",
      "Processing row 3900\n",
      "Processing row 4200\n",
      "Processing row 4500\n",
      "Processing row 4800\n",
      "Processing row 5100\n"
     ]
    }
   ],
   "source": [
    "# # use word counts\n",
    "# word_counts = pd.DataFrame(columns=common_words)\n",
    "# i = 0\n",
    "# for sentence in sentences.iloc[:,0]:\n",
    "#     # repeat this for every sentence\n",
    "#     x = pd.Series(sentence).value_counts()\n",
    "#     x = x[pd.Series([str(x) for x in x.index]).isin(common_words).values]\n",
    "#     word_counts.loc[i,:] = 0\n",
    "#     word_counts.loc[i,[str(x) for x in x.index]] = x.values\n",
    "#     i += 1\n",
    "#     if i % 250 == 0:\n",
    "#             print(\"Processing row {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:17:14.503905Z",
     "start_time": "2019-02-13T02:02:04.191512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 250\n",
      "Processing row 500\n",
      "Processing row 750\n",
      "Processing row 1000\n",
      "Processing row 1250\n",
      "Processing row 1500\n",
      "Processing row 1750\n",
      "Processing row 2000\n",
      "Processing row 2250\n",
      "Processing row 2500\n",
      "Processing row 2750\n",
      "Processing row 3000\n",
      "Processing row 3250\n",
      "Processing row 3500\n",
      "Processing row 3750\n",
      "Processing row 4000\n",
      "Processing row 4250\n",
      "Processing row 4500\n",
      "Processing row 4750\n",
      "Processing row 5000\n",
      "Processing row 5250\n"
     ]
    }
   ],
   "source": [
    "lemma_counts = bag_of_lemma(sentences.iloc[:,0], common_token_lemmas, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T23:12:52.137706Z",
     "start_time": "2019-02-12T23:01:31.385150Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n",
      "Processing row 3000\n",
      "Processing row 3100\n",
      "Processing row 3200\n",
      "Processing row 3300\n",
      "Processing row 3400\n",
      "Processing row 3500\n",
      "Processing row 3600\n",
      "Processing row 3700\n",
      "Processing row 3800\n",
      "Processing row 3900\n",
      "Processing row 4000\n",
      "Processing row 4100\n",
      "Processing row 4200\n",
      "Processing row 4300\n",
      "Processing row 4400\n",
      "Processing row 4500\n",
      "Processing row 4600\n",
      "Processing row 4700\n",
      "Processing row 4800\n",
      "Processing row 4900\n",
      "Processing row 5000\n",
      "Processing row 5100\n",
      "Processing row 5200\n",
      "Processing row 5300\n"
     ]
    }
   ],
   "source": [
    "# # use word counts\n",
    "# lemma_counts = pd.DataFrame(columns=common_token_lemmas)\n",
    "# i = 0\n",
    "# for sentence in sentences.iloc[:,0]:\n",
    "#     # repeat this for every sentence\n",
    "#     x = pd.Series(0sentence.lemma_.split(' ')).value_counts()\n",
    "#     x = x[pd.Series([str(x) for x in x.index]).isin(common_token_lemmas).values]\n",
    "#     lemma_counts.loc[i,:] = 0\n",
    "#     lemma_counts.loc[i,[str(x) for x in x.index]] = x.values\n",
    "#     i += 1\n",
    "#     if i % 250 == 0:\n",
    "#             print(\"Processing row {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:17:34.845196Z",
     "start_time": "2019-02-13T02:17:32.974817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.977742946708464\n",
      "\n",
      "Test set score: 0.7570488721804511\n"
     ]
    }
   ],
   "source": [
    "# create target variable\n",
    "# Here 0 indicates that Caroll was the author\n",
    "# and if 1 then Austen is the author\n",
    "Y = (sentences.author == 'Austen').astype(int).values\n",
    "X = word_counts.astype(int).values\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:17:50.518732Z",
     "start_time": "2019-02-13T02:17:45.519237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9937304075235109\n",
      "\n",
      "Test set score: 0.8834586466165414\n"
     ]
    }
   ],
   "source": [
    "# now use lemma instead of token\n",
    "# create target variable\n",
    "Y_lemma = Y\n",
    "X_lemma = lemma_counts.astype(int).values\n",
    "\n",
    "rfc_lemma = ensemble.RandomForestClassifier()\n",
    "\n",
    "X_lemma_train, X_lemma_test, y_lemma_train, y_lemma_test = train_test_split(X_lemma, \n",
    "                                                    Y_lemma,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train_lemma = rfc_lemma.fit(X_lemma_train, y_lemma_train)\n",
    "\n",
    "print('Training set score:', rfc_lemma.score(X_lemma_train, y_lemma_train))\n",
    "print('\\nTest set score:', rfc_lemma.score(X_lemma_test, y_lemma_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\n",
    "\n",
    "\n",
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:18:08.704492Z",
     "start_time": "2019-02-13T02:18:08.473458Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 3062) (3190,)\n",
      "Training set score: 0.9178683385579938\n",
      "\n",
      "Test set score: 0.8369360902255639\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:18:22.790421Z",
     "start_time": "2019-02-13T02:18:22.109747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 7146) (3190,)\n",
      "Training set score: 0.9733542319749217\n",
      "\n",
      "Test set score: 0.943609022556391\n"
     ]
    }
   ],
   "source": [
    "lr_lemma = LogisticRegression()\n",
    "train_lemma = lr_lemma.fit(X_lemma_train, y_lemma_train)\n",
    "print(X_lemma_train.shape, y_lemma_train.shape)\n",
    "print('Training set score:', lr_lemma.score(X_lemma_train, y_lemma_train))\n",
    "print('\\nTest set score:', lr_lemma.score(X_lemma_test, y_lemma_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:19:15.310165Z",
     "start_time": "2019-02-13T02:18:37.230447Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8213166144200627\n",
      "\n",
      "Test set score: 0.7800751879699248\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:23:28.729937Z",
     "start_time": "2019-02-13T02:22:10.280092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9282131661442006\n",
      "\n",
      "Test set score: 0.9295112781954887\n"
     ]
    }
   ],
   "source": [
    "clf_lemma = ensemble.GradientBoostingClassifier()\n",
    "train_lemma = clf_lemma.fit(X_lemma_train, y_lemma_train)\n",
    "\n",
    "print('Training set score:', clf_lemma.score(X_lemma_train, y_lemma_train))\n",
    "print('\\nTest set score:', clf_lemma.score(X_lemma_test, y_lemma_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement.\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "What if we feed the model a different novel by Jane Austen, like _Emma_?  Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "\n",
    "First, we need to process _Emma_ the same way we processed the other data, and combine it with the Alice data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:25:07.502856Z",
     "start_time": "2019-02-13T02:24:26.310843Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])\n",
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)\n",
    "emma_sents = [sent for sent in emma_doc.sents]\n",
    "# instead of truncating the sentences at the total number in Allice In Wonderland,\n",
    "# let's sample from the entire text instead. \n",
    "# It is my belief that the other technique introduces bias.\n",
    "emma_sents = random.sample(emma_sents, len(alice_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:27:05.562367Z",
     "start_time": "2019-02-13T02:26:34.828880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 250\n",
      "Processing row 500\n",
      "Processing row 750\n",
      "Processing row 1000\n",
      "Processing row 1250\n",
      "Processing row 1500\n"
     ]
    }
   ],
   "source": [
    "word_counts_emma = my_bag_of_words(emma_sents, common_words, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:49:19.311093Z",
     "start_time": "2019-02-13T02:49:19.022726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.9046615749275744\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>431</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>137</td>\n",
       "      <td>3004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0     1\n",
       "row_0           \n",
       "0.0    431   225\n",
       "1.0    137  3004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emma_test = np.concatenate((X_test, word_counts_emma.astype(int).values))\n",
    "y_emma_test = np.concatenate((y_test, np.ones((word_counts_emma.shape[0], ))))\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_emma_test, y_emma_test))\n",
    "lr_emma_predicted = lr.predict(X_emma_test)\n",
    "pd.crosstab(y_emma_test, lr_emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Well look at that!  NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland.  Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\n",
    "\n",
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%.  See what you can do to improve performance.  Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires.  Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%.  \n",
    "\n",
    "An easy way to improve the overall accuracy is to use **lemma counts**, not just token/word counts. This will increase our feature space from **3062 to 7146**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T02:52:35.539155Z",
     "start_time": "2019-02-13T02:52:34.647349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.9515406900184356\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>560</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>88</td>\n",
       "      <td>3053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0     1\n",
       "row_0           \n",
       "0.0    560    96\n",
       "1.0     88  3053"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_emma_counts = bag_of_lemma(emma_sents, common_token_lemmas)\n",
    "# add the emma stuff to the X_test and y_test matrices\n",
    "X_lemma_emma_test = np.concatenate((X_lemma_test, lemma_emma_counts.astype(int).values))\n",
    "y_lemma_emma_test = np.concatenate((y_lemma_test, np.ones((lemma_emma_counts.shape[0], ))))\n",
    "# Model.\n",
    "print('\\nTest set score:', lr_lemma.score(X_lemma_emma_test, y_lemma_emma_test))\n",
    "lr_emma_lemma_predicted = lr_lemma.predict(X_lemma_emma_test)\n",
    "pd.crosstab(y_lemma_emma_test, lr_emma_lemma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All I did here was use lemma counts instead of token (word) counts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:06:54.825095Z",
     "start_time": "2019-02-13T05:06:10.222201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9570713721358968"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "svc_lemma = SVC(C=0.05, kernel='linear', random_state=0)\n",
    "svc_lemma.fit(X_lemma_train, y_lemma_train)\n",
    "svc_lemma.score(X_lemma_emma_test, y_lemma_emma_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T00:59:29.723190Z",
     "start_time": "2019-02-13T00:59:29.720557Z"
    }
   },
   "source": [
    "**XGBoost**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T04:02:24.265450Z",
     "start_time": "2019-02-13T03:59:43.215016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363148479427549"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X_lemma_train, y_lemma_train)\n",
    "xgbc.score(X_lemma_emma_test, y_lemma_emma_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:07:01.352076Z",
     "start_time": "2019-02-13T05:07:00.944150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328417171451145"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_lemma_train, y_lemma_train)\n",
    "nb.score(X_lemma_emma_test, y_lemma_emma_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cases, our models were trained on 3190 sentences and used to predict 3797 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.  This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "\n",
    "Record your work for each challenge in a notebook and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T01:29:30.952812Z",
     "start_time": "2019-02-13T01:29:30.947652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T04:18:11.448903Z",
     "start_time": "2019-02-13T04:16:05.349756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book I Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought \n",
      "\n",
      "Test set score: 0.8964972346589413\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>560</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>297</td>\n",
       "      <td>2844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0     1\n",
       "row_0           \n",
       "0.0    560    96\n",
       "1.0    297  2844"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "milton = gutenberg.raw('milton-paradise.txt')\n",
    "milton = re.sub(r'VOLUME \\w+', '', milton)\n",
    "milton = re.sub(r'CHAPTER \\w+', '', milton)\n",
    "milton = text_cleaner(milton)\n",
    "print(milton[:100])\n",
    "# Parse our cleaned data.\n",
    "milton_doc = nlp(milton)\n",
    "milton_sents = [sent for sent in milton_doc.sents]\n",
    "# instead of truncating the sentences at the total number in Allice In Wonderland,\n",
    "# let's sample from the entire text instead. \n",
    "# It is my belief that the other technique introduces bias.\n",
    "milton_sents = random.sample(milton_sents, len(alice_sents))\n",
    "\n",
    "lemma_milton_counts = bag_of_lemma(milton_sents, common_token_lemmas)\n",
    "# add the emma stuff to the X_test and y_test matrices\n",
    "X_lemma_milton_test = np.concatenate((X_lemma_test, lemma_milton_counts.astype(int).values))\n",
    "y_lemma_milton_test = np.concatenate((y_lemma_test, np.ones((lemma_milton_counts.shape[0], ))))\n",
    "# Model.\n",
    "print('\\nTest set score:', lr_lemma.score(X_lemma_milton_test, y_lemma_milton_test))\n",
    "lr_milton_lemma_predicted = lr_lemma.predict(X_lemma_milton_test)\n",
    "pd.crosstab(y_lemma_milton_test, lr_milton_lemma_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T05:11:21.602526Z",
     "start_time": "2019-02-13T05:10:37.562121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362654727416382"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVC\n",
    "svc_lemma_milton = SVC(C=0.05, kernel='linear', random_state=0)\n",
    "svc_lemma_milton.fit(X_lemma_train, y_lemma_train)\n",
    "svc_lemma_milton.score(X_lemma_milton_test, y_lemma_milton_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion  \n",
    "\n",
    "Just tokenization resulted in poor performance compared to lemmatization. This simply change resulted in multiple models that achieved over 90% accuracy. With a little tuning, an SVM classifier was the most accurate (on the test set) at 95.7%. The features that I engineered did not add any predictive power, so I did not include that code in this notebook but is available on request. Those features included: sentence length, average word length, POS tagging (nouns, verbs, adjectives, etc.), polarity (negative, neutral, positive, compound).  \n",
    "\n",
    "It should be noted that a Naive Bayes classifier worked very well for this data and was especially fast to train. The logistic regression model was also quick to train and was the very accurate. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
