{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T23:27:56.352112Z",
     "start_time": "2019-03-01T23:27:52.773248Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "random.seed(42)\n",
    "from nltk.corpus import stopwords, state_union\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor downloading Corpora\\nimport nltk\\nnltk.download()\\n\\nfor download english langugage in spaCy\\n!python -m spacy download en\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For downloading Corpora\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "for download english langugage in spaCy\n",
    "!python -m spacy download en\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "\n",
    "In this challenge, I want to build a model to classify state of the union addresses. In this case I will only be concerned with two previous presidents: Ronald Reagan and Bill Clinton; given a single sentence can we predict which president said it? This project will involve 5 primary steps:\n",
    "\n",
    "  1. Data cleaning / processing / language parsing\n",
    "  2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "  3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "  4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "  5. Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "  \n",
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:13:32.011416Z",
     "start_time": "2019-03-01T21:13:31.998530Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1.  Get all Reagan transcripts\n",
    "reagan_text = \"\"\n",
    "files = ['1981-Reagan.txt', '1982-Reagan.txt',\n",
    "         '1983-Reagan.txt','1984-Reagan.txt',\n",
    "         '1985-Reagan.txt','1986-Reagan.txt',\n",
    "         '1987-Reagan.txt','1988-Reagan.txt']\n",
    "\n",
    "for fname in files:\n",
    "    reagan_text += state_union.raw(fname).replace('\\n',' ').lower()\n",
    "    \n",
    "# Bill Clinton\n",
    "\n",
    "# 1.  Get all Reagan transcripts\n",
    "files = ['1993-Clinton.txt', '1994-Clinton.txt', \n",
    "         '1995-Clinton.txt', '1996-Clinton.txt', \n",
    "         '1997-Clinton.txt', '1998-Clinton.txt', \n",
    "         '1999-Clinton.txt', '2000-Clinton.txt',]\n",
    "clinton_text = \"\"\n",
    "\n",
    "for fname in files:\n",
    "    clinton_text += state_union.raw(fname).replace('\\n',' ').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP Raw Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:13:45.773410Z",
     "start_time": "2019-03-01T21:13:34.461607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "reagan_doc = nlp(reagan_text)\n",
    "clinton_doc = nlp(clinton_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Break into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:13:46.157800Z",
     "start_time": "2019-03-01T21:13:46.107613Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "reagan_sents = [sent.string for sent in reagan_doc.sents]\n",
    "clinton_sents = [sent.string for sent in clinton_doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 1,883 total sentences for Reagan and 3,183 sentences from Clinton, so we need to resample Clinton's sentences so that they are of the same sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "clinton_sents = random.sample(clinton_sents, len(reagan_sents))\n",
    "\n",
    "sentences = reagan_sents + clinton_sents\n",
    "\n",
    "a = np.zeros((len(reagan_sents),))\n",
    "b = np.ones((len(clinton_sents), ))\n",
    "target = np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_text(text):\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split(' ') if word not in STOPWORDS and \n",
    "              word != '' and not word.isnumeric()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = [tokenize_text(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T02:59:11.755519Z",
     "start_time": "2019-03-01T02:59:11.709341Z"
    }
   },
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:14:12.806801Z",
     "start_time": "2019-03-01T21:14:12.076144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year          474\n",
       "american      351\n",
       "people        345\n",
       "america       319\n",
       "new           241\n",
       "government    237\n",
       "work          203\n",
       "child         198\n",
       "world         188\n",
       "congress      183\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english') #, min_df=10, max_df=.05)\n",
    "bow = cv.fit_transform(sentences, target).toarray()\n",
    "bow = pd.DataFrame(bow, columns=cv.get_feature_names())\n",
    "bow.sum(axis=0).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:14:35.034683Z",
     "start_time": "2019-03-01T21:14:34.280787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year          82.611723\n",
       "american      68.165984\n",
       "people        67.440010\n",
       "america       63.923792\n",
       "let           49.954837\n",
       "work          47.763782\n",
       "government    47.491771\n",
       "new           45.238571\n",
       "child         42.333046\n",
       "time          41.783926\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tcv = TfidfVectorizer(stop_words='english') #, min_df=10, max_df=.05)\n",
    "tfidf = tcv.fit_transform(sentences, target).toarray()\n",
    "tfidf = pd.DataFrame(tfidf, columns=tcv.get_feature_names())\n",
    "\n",
    "tfidf.sum(axis=0).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split** data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:14:47.336479Z",
     "start_time": "2019-03-01T21:14:46.833206Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "X_bow_train, X_bow_test, y_train, y_test = tts(bow, \n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "X_tfidf_train, X_tfidf_test, y_train, y_test = tts(tfidf, \n",
    "                                                    target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models  \n",
    "\n",
    "## Bag of Words\n",
    "  1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:14:51.449434Z",
     "start_time": "2019-03-01T21:14:50.682646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3012, 5112) (3012,)\n",
      "Training set score: 0.9309428950863213\n",
      "\n",
      "Test set score: 0.7214854111405835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_bow_train, y_train)\n",
    "print(X_bow_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_bow_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_bow_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, logistic regression overfits the training data big time! SVM's should help with this.     \n",
    "  2. Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:19:27.697441Z",
     "start_time": "2019-03-01T21:19:27.494253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3012, 5102) (3012,)\n",
      "Training set score: 0.8841301460823373\n",
      "\n",
      "Test set score: 0.7360742705570292\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression()\n",
    "train = lr2.fit(X_tfidf_train, y_train)\n",
    "print(X_tfidf_train.shape, y_train.shape)\n",
    "print('Training set score:', lr2.score(X_tfidf_train, y_train))\n",
    "print('\\nTest set score:', lr2.score(X_tfidf_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As to be expected, using tf-idf results in less overfitting, but is only slightly more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec\n",
    "\n",
    "Similar to word2vec but operates on a document basis, see more [here](https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tagged_data_reagan = [TaggedDocument(words=word_tokenize(sent.string), tags=[0]) for sent in reagan_doc.sents]\n",
    "tagged_data_clinton = [TaggedDocument(words=word_tokenize(sent.string), tags=[1]) for sent in clinton_doc.sents]\n",
    "tagged_data_clinton = random.sample(tagged_data_clinton, len(tagged_data_reagan))\n",
    "\n",
    "tagged_data = tagged_data_clinton + tagged_data_reagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 10\n",
      "iteration 20\n",
      "iteration 30\n",
      "iteration 40\n",
      "iteration 50\n",
      "iteration 60\n",
      "iteration 70\n",
      "iteration 80\n",
      "iteration 90\n",
      "iteration 100\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 110\n",
    "vec_size = 10\n",
    "alpha = 0.045\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    if (epoch % 10)==0:\n",
    "        print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0004\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.72\n",
      "Test score: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "reagan_sents = [sent.string for sent in reagan_doc.sents]\n",
    "clinton_sents = [sent.string for sent in clinton_doc.sents]\n",
    "clinton_sents = random.sample(clinton_sents, len(reagan_sents))\n",
    "\n",
    "sentences = reagan_sents + clinton_sents\n",
    "\n",
    "a = np.zeros((len(reagan_sents),))\n",
    "b = np.ones((len(clinton_sents), ))\n",
    "target = np.concatenate((a, b), axis=0)\n",
    "\n",
    "embeddings = np.zeros((len(sentences), vec_size))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    embeddings[i] = model.infer_vector(word_tokenize(sentence))\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(embeddings, target, \n",
    "                                       test_size=.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Train score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
