{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:32:28.259388Z",
     "start_time": "2019-06-23T20:32:27.404402Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import warnings\n",
    "rand_state = 42\n",
    "np.random.seed(rand_state)\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:32:39.468797Z",
     "start_time": "2019-06-23T20:32:38.885560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (229, 4)\n"
     ]
    }
   ],
   "source": [
    "path = 'sotu'\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "df = pd.DataFrame(columns=['year', 'president', 'party', 'text'])\n",
    "\n",
    "for i in range(len(dirs)):\n",
    "    components = dirs[i].split('_')\n",
    "    name = components[0]\n",
    "    year = components[1].split('.')[0]\n",
    "    df.loc[i,'year'] = year\n",
    "    df.loc[i,'president'] = name   \n",
    "    \n",
    "    filename = os.path.join(path, dirs[i])\n",
    "    text_file = open(filename, \"r\")\n",
    "    \n",
    "    lines = text_file.read()\n",
    "    lines = lines.replace('\\n', ' ')\n",
    "    df.loc[i, 'text'] = lines.lower()\n",
    "    \n",
    "df.year = df.year.astype(int) \n",
    "df.president = df.president.astype(str)\n",
    "df.text = df.text.astype(str)\n",
    "print('Shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate\n",
    "\n",
    "There are a few presidents that have the same last name (Roosevelt, Bush, Johnson and Adams), so let's clean that up now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:32:41.570132Z",
     "start_time": "2019-06-23T20:32:41.517207Z"
    }
   },
   "outputs": [],
   "source": [
    "# need to distinuish between Theodore Roosevelt and Franklin D. Roosevelt\n",
    "indices = df.query(\"president =='Roosevelt' & year <= 1909\").index\n",
    "df.loc[indices,'president'] = 'Theodore Roosevelt'\n",
    "\n",
    "indices = df.query(\"president == 'Roosevelt'\").index\n",
    "df.loc[indices,'president'] = 'Franklin D. Roosevelt'\n",
    "\n",
    "indices = df.query(\"president =='Bush' & year <= 1992\").index\n",
    "df.loc[indices,'president'] = 'George H. W. Bush'\n",
    "\n",
    "indices = df.query(\"president == 'Bush'\").index\n",
    "df.loc[indices,'president'] = 'George W. Bush'\n",
    "\n",
    "indices = df.query(\"president =='Johnson' & year <= 1869\").index\n",
    "df.loc[indices,'president'] = 'Andrew Johnson'\n",
    "\n",
    "indices = df.query(\"president == 'Johnson'\").index\n",
    "df.loc[indices,'president'] = 'Lyndon B. Johnson'\n",
    "\n",
    "indices = df.query(\"president =='Adams' & year <= 1801\").index\n",
    "df.loc[indices,'president'] = 'John Adams'\n",
    "\n",
    "indices = df.query(\"president == 'Adams'\").index\n",
    "df.loc[indices,'president'] = 'John Quincy Adams'\n",
    "\n",
    "\n",
    "indices = df.query(\"president =='Harrison' & year <= 1841\").index\n",
    "df.loc[indices,'president'] = 'William Henry Harrison'\n",
    "\n",
    "indices = df.query(\"president == 'Harrison'\").index\n",
    "df.loc[indices,'president'] = 'Benjamin Harrison'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's attach the political party to the president (note some presidents changed parties throughout their political career, I am simply listing the party they belonged to while serving as President)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:32:43.135321Z",
     "start_time": "2019-06-23T20:32:43.110744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president</th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>fellow-citizens of the senate and house of rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>fellow-citizens of the senate and house of rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>fellow-citizens of the senate and house of rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>fellow-citizens of the senate and house of rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>Washington</td>\n",
       "      <td>Unaffiliated</td>\n",
       "      <td>fellow-citizens of the senate and house of rep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       president         party  \\\n",
       "year                             \n",
       "1791  Washington  Unaffiliated   \n",
       "1792  Washington  Unaffiliated   \n",
       "1793  Washington  Unaffiliated   \n",
       "1794  Washington  Unaffiliated   \n",
       "1795  Washington  Unaffiliated   \n",
       "\n",
       "                                                   text  \n",
       "year                                                     \n",
       "1791  fellow-citizens of the senate and house of rep...  \n",
       "1792  fellow-citizens of the senate and house of rep...  \n",
       "1793  fellow-citizens of the senate and house of rep...  \n",
       "1794  fellow-citizens of the senate and house of rep...  \n",
       "1795  fellow-citizens of the senate and house of rep...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pres_to_party(name):\n",
    "    republican = ['Lincoln', 'Grant', 'Hayes', 'Garfield', 'Arthur', \n",
    "                  'Benjamin Harrison', 'McKinley', 'Theodore Roosevelt', \n",
    "                  'Taft', 'Harding', 'Coolidge', 'Hoover', 'Eisenhower', \n",
    "                  'Nixon', 'Ford', 'Reagan', 'George H. W. Bush', \n",
    "                  'George W. Bush', 'Trump']\n",
    "    if name in republican:\n",
    "        return 'Republican'\n",
    "    \n",
    "    democratic = ['Jackson', 'Buren', 'Polk', 'Pierce', \n",
    "                  'Buchanan', 'Cleveland', 'Wilson', 'Franklin D. Roosevelt', \n",
    "                  'Truman', 'Kennedy', 'Lyndon B. Johnson', 'Carter', 'Clinton', 'Obama']\n",
    "    if name in democratic:\n",
    "        return 'Democratic'\n",
    "    \n",
    "    whig = ['William Henry Harrison', 'Taylor', 'Fillmore']\n",
    "    if name in whig:\n",
    "        return 'Whig'\n",
    "    \n",
    "    national_union = ['Andrew Johnson']\n",
    "    if name in national_union:\n",
    "        return 'National Union'\n",
    "    \n",
    "    \n",
    "    unaffiliated = ['Washington', 'Tyler']\n",
    "    if name in unaffiliated:\n",
    "        return 'Unaffiliated'\n",
    "    \n",
    "    federalist = ['John Adams']\n",
    "    if name in federalist:\n",
    "        return 'Federalist'\n",
    "    \n",
    "    democratic_republican = ['Jefferson', 'Madison', 'Monroe', 'John Quincy Adams']\n",
    "    if name in democratic_republican:\n",
    "        return 'Democratic-Republican'\n",
    "    \n",
    "df.party = df.president.apply(pres_to_party)\n",
    "\n",
    "df.set_index('year', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# need to drop George Washington's 1790 address as the file is empty\n",
    "df = df.iloc[1:,:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the political parties are not well balanced, the Democrats and Republicans accounnt for 177 out of 227 addresses, while the other three parties made up only 22%. Therefore, it might be more interesting to predict the probability that a given text belongs to a particular political party, eg. 62% Democrat and 38% Republican."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:32:45.410639Z",
     "start_time": "2019-06-23T20:32:45.406200Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df.party.isin(['Republican', 'Democratic'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize  \n",
    "\n",
    "We will begin by splitting our text (single SOTU transcript) into it's constituent sentences, and then split each sentence into words, only keeping alphanumeric characters, tag each word with it's part of speech, and then lemmatize each word (Word net lemmatizer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:34:33.523421Z",
     "start_time": "2019-06-23T20:34:29.962243Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = [sent_tokenize(text) for text in df.text]\n",
    "\n",
    "# remove the first and last sentences (meaningless intro/closing statements)\n",
    "for i in range(len(sentences)):\n",
    "    del sentences[i][0]\n",
    "    del sentences[i][-1]  \n",
    "    \n",
    "sentence_lengths = [len(sent) for sent in sentences]\n",
    "df['sentences'] = sentences\n",
    "df['sentence_length'] = [len(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:34:35.500280Z",
     "start_time": "2019-06-23T20:34:35.154901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mitchellmurphy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:34:38.442077Z",
     "start_time": "2019-06-23T20:34:38.437968Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    words = [word for word in sentence.split(' ')] # if word not in STOPWORDS]\n",
    "    _s = ' '.join(words)\n",
    "    _s = re.sub('[\\.,\\?\\!]','',_s)\n",
    "    _s = re.sub('\\d+', '_NUMBER',_s)\n",
    "    return _s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:34:42.742894Z",
     "start_time": "2019-06-23T20:34:42.735712Z"
    }
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "def clean_text(text):\n",
    "    #text = text.lower().replace(',','')\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    cleaned_text = [t for t in tokenized_text if t not in stop_words] # and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)\n",
    "    _l = lemmatize_sent(' '.join(cleaned_text))\n",
    "    # replace numbers with _NUMBER\n",
    "    #_l = re.sub('\\d+', '_NUMBER',' '.join(_l))\n",
    "    # strip punctuation\n",
    "    _l = re.sub('[\\.,\\?\\!]','',' '.join(_l))\n",
    "    _l = re.sub('[^a-z0-9]', ' ', _l)\n",
    "    return _l.replace('  ',' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:38:06.951394Z",
     "start_time": "2019-06-23T20:35:19.231111Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences_all = []\n",
    "for sentence in sentences:\n",
    "    for _s in sentence:\n",
    "        sentences_all.append(clean_text(_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:42:05.804716Z",
     "start_time": "2019-06-23T20:42:05.726156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012824297892755656"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t = 0\n",
    "for _s in sentences_all:\n",
    "    if len(_s.split()) <= 1:\n",
    "        _t += 1\n",
    "_t/len(sentences_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:41:53.181493Z",
     "start_time": "2019-06-23T20:41:53.177162Z"
    }
   },
   "source": [
    "So 1.2\\% of all sentences only contain 1 word. Not great but pretty small fraction so for now we will keep them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed our text, lets create target labels, merge them into a dataframe and save it for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:42:51.097834Z",
     "start_time": "2019-06-23T20:42:51.068099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>task devolves provision constitution present f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>communicate first time source unfeigned satisf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>turn eye nation great desire see brethren huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>foreign relation although general character pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>effect adjustment shall continue object earnes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  task devolves provision constitution present f...\n",
       "1      0  communicate first time source unfeigned satisf...\n",
       "2      0  turn eye nation great desire see brethren huma...\n",
       "3      0  foreign relation although general character pa...\n",
       "4      0  effect adjustment shall continue object earnes..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use Democratic as the positive class\n",
    "df['party_num'] = (df.party == 'Democratic').astype(int)\n",
    "\n",
    "target = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    target.append((np.ones((df.iloc[i,4],)) * int(df.iloc[i,1] == 'Republican')))\n",
    "    \n",
    "target = np.concatenate(target, axis=0)\n",
    "\n",
    "df_processed = pd.DataFrame({'label': target, 'text': sentences_all})\n",
    "df_processed.label = df_processed.label.astype(int)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:43:12.685654Z",
     "start_time": "2019-06-23T20:43:12.413285Z"
    }
   },
   "outputs": [],
   "source": [
    "df_processed.to_csv('sotu_lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trump 2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sotu/Trump_2019.txt\"\n",
    "text_file = open(filename, \"r\")\n",
    "\n",
    "lines = text_file.read()\n",
    "lines = lines.replace('\\n', ' ')\n",
    "text = lines.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "#sentences = [sent_tokenize(_t) for _t in text]\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.count('applause') / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = [len(sentence.split()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove applause\n",
    "text = text.replace('(applause.)','')\n",
    "# remove commas\n",
    "text = text.replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sentences[0]\n",
    "del sentences[-1]\n",
    "\n",
    "sentences_all = []\n",
    "for sentence in sentences:\n",
    "    sentences_all.append(clean_text(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_all = [_s for _s in sentences_all if len(_s.split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = len(sentences_all)\n",
    "\n",
    "target = np.zeros((num_sentences))\n",
    "\n",
    "df_processed = pd.DataFrame({'label': target, 'text': sentences_all})\n",
    "df_processed.label = df_processed.label.astype(int)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv('trump_2019_lemmatized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
